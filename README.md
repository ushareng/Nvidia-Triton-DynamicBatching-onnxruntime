# Triton Dynamic Batching ONNX Demo (CPU)

This repository demonstrates **dynamic batching** using **NVIDIA Triton Inference Server** on a **CPU-only Ubuntu VM**, without Docker or GPUs.

The demo uses a minimal **ONNX Runtime** model that doubles the input tensor values and shows how Triton batches multiple concurrent requests into fewer executions.

<img width="1907" height="256" alt="image" src="https://github.com/user-attachments/assets/d332f42a-28e8-4755-a0db-91516f450292" />

---

## ğŸš€ What This Demo Shows

- Running Triton Inference Server **natively on Ubuntu**
- Deploying an **ONNX model** using the `onnxruntime` backend
- Enabling and validating **dynamic batching**
- Sending **concurrent gRPC requests**
- Verifying batching behavior via **Prometheus metrics**

---

## ğŸ§  Model Overview

- **Model name:** `onnx_double`
- **Backend:** ONNX Runtime
- **Operation:** `OUTPUT = INPUT * 2`
- **Input shape:** `[-1, 4]` (dynamic batch dimension)
- **Device:** CPU

---

## ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ model_repository/
â”‚   â””â”€â”€ onnx_double/
â”‚       â”œâ”€â”€ 1/
â”‚       â”‚   â””â”€â”€ model.onnx
â”‚       â””â”€â”€ config.pbtxt
â”œâ”€â”€ client_dynbatch_onnx.py
â”œâ”€â”€ generate_onnx_model.py
â””â”€â”€ README.md
```

---

## âš™ï¸ Dynamic Batching Configuration

```pbtxt
dynamic_batching {
  preferred_batch_size: [ 2, 4, 8 ]
  max_queue_delay_microseconds: 2000
}
```

---

## â–¶ï¸ Running Triton Server

```bash
./bin/tritonserver   --model-repository=$HOME/model_repository   --http-port=8000   --grpc-port=8001   --metrics-port=8002
```

---

## ğŸ§ª Running the Client

```bash
pip install -U tritonclient[grpc] numpy
python3 client_dynbatch_onnx.py
```

---

## ğŸ“Š Verify Dynamic Batching

```bash
curl localhost:8002/metrics | grep onnx_double
```

Look for:
- `nv_inference_request_success`
- `nv_inference_exec_count` < request count

---

## ğŸ“Œ Requirements

- Ubuntu 22.04+
- Python 3.10+
- Triton Inference Server 2.64.0
- CPU-only (no GPU required)

---

## ğŸ“„ License

This project is licensed under the MIT License.

Â© 2026 Usha Rengaraju

See the [LICENSE](LICENSE) file for full details.
